{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pokémon Diffusion<a id=\"top\"></a>**\n",
    "\n",
    "> #### ``02-Diffusion-Model.ipynb``\n",
    "\n",
    "<i><small>**Alumno:** Alejandro Pequeño Lizcano<br>Última actualización: 18/03/2024</small></i></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: introducir el mdoelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports\n",
    "\n",
    "Una vez introducido el proyecto, se importan las librerías necesarias para el desarrollo de este apartado.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 19:57:02.571504: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-21 19:57:02.603353: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-21 19:57:03.098730: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'paths'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m path_loader \u001b[38;5;28;01mas\u001b[39;00m pl\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_dataset\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvisualization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m visualize\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DiffusionModel\n",
      "File \u001b[0;32m/mnt/c/Users/pqlza/UPM/TFG/DiffusionScratch/src/visualization/visualize.py:16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiffusion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DiffusionModel\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m onehot_to_string\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# =====================================================================\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Plotting functions\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# =====================================================================\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Plot random images from a list or dictionary of image paths\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# =====================================================================\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Users/pqlza/UPM/TFG/DiffusionScratch/src/model/diffusion.py:16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m string_to_onehot, onehot_to_string\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Set up\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# =====================================================================\u001b[39;00m\n\u001b[1;32m     20\u001b[0m config \u001b[38;5;241m=\u001b[39m configparser\u001b[38;5;241m.\u001b[39mConfigParser()\n",
      "File \u001b[0;32m/mnt/c/Users/pqlza/UPM/TFG/DiffusionScratch/src/utils/utils.py:19\u001b[0m\n\u001b[1;32m     16\u001b[0m config \u001b[38;5;241m=\u001b[39m configparser\u001b[38;5;241m.\u001b[39mConfigParser()\n\u001b[1;32m     17\u001b[0m config\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../config.ini\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m DATA_PATH \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpaths\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     20\u001b[0m poke_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/raw/pokedex.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Functions\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# =====================================================================\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# TODO: Adapt this to the new dataset with the new types (type1 and type2)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/configparser.py:979\u001b[0m, in \u001b[0;36mRawConfigParser.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m    978\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_section \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_section(key):\n\u001b[0;32m--> 979\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    980\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_proxies[key]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'paths'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "# =====================================================================\n",
    "import re\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Import libraries for data preprocessing\n",
    "import configparser\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Import src code\n",
    "from src.data import path_loader as pl\n",
    "from src.data import create_dataset\n",
    "from src.visualization import visualize\n",
    "from src.model import DiffusionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set config file\n",
    "# =====================================================================\n",
    "SETTINGS_PATH = \"../config.ini\"\n",
    "config = configparser.ConfigParser()\n",
    "config.read(SETTINGS_PATH)\n",
    "\n",
    "config_paths = config[\"paths\"]\n",
    "data_path = config_paths[\"data_path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "# =====================================================================\n",
    "config_hp = config[\"hyperparameters\"]\n",
    "\n",
    "IMG_SIZE = int(config_hp[\"img_size\"])\n",
    "NUM_CLASSES = int(config_hp[\"num_classes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de Difusión\n",
    "\n",
    "TODO: Introducir mejor\n",
    "\n",
    "El modelo usado para la predicción del ruido. Según muchos papers, se podría usar cualquier red neuronal, ya que no existe una arquitectura específica y depende del conjunto de datos con el que se entrene, no obstante, la más usada para la síntesis de imágenes y la que se ha usado en este proyecto es la arquitectura **U-Net** por sus características de recuperación de la información manteniendo la dimensionalidad de la imagen.\n",
    "\n",
    "Esta arquitectura se caracteriza por tener una parte de codificación y una parte de decodificación. La parte de codificación se encarga de reducir la dimensionalidad de la imagen de entrada y la parte de decodificación se encarga de reconstruir la imagen original a partir de la imagen codificada.\n",
    "\n",
    "Podemos encontrar dos funciones, por una parte, ``block()``, que se encarga de definir el bloque de difusión y, por otra parte, ``build_ddpm_model()``, que se encarga de definir la arquitectura **U-Net**.\n",
    "\n",
    "- ``block()``: El bloque de difusión contiene tres parámetros:\n",
    "\n",
    "    - ``x_parameter``: es el tensor de entrada que contiene la imagen original o la imagen con ruido en cada paso de difusión.\n",
    "    - ``time_parameter`` es el tensor que indica el paso de difusión en el que nos encontramos. Se usa para calcular el valor de **$\\beta$** según el *scheduler* que hayamos elegido.\n",
    "\n",
    "    Dentro del bloque de difusión, se aplican transformaciones a cada uno de estos parámetros, lo que puede incluir capas densas, normalización y activación ReLU. Estas transformaciones capturan las relaciones y dependencias entre los diferentes aspectos de la entrada (imagen y tiempo). Finalmente, se calcula la imagen nueva con el ruido añadido.\n",
    "\n",
    "-  El proceso de difusión utiliza una arquitectura de tipo **U-Net** modificada con bloques ``block`` que toman en cuenta la imagen (``x_parameter``) y el tensor tiempo (``time_parameter``). Posteriormente, se realizan operaciones de convolución y pooling para reducir la resolución de la imagen mientras se procesa la información temporal. Luego, se realiza un proceso de decodificación utilizando operaciones de upsampling y concatenación para generar una imagen de salida que tiene la misma resolución que la imagen de entrada. Después de este proceso, se añade una capa **MLP** para procesar la información temporal y generar una imagen de salida. Finalmente, se devuelve la imagen de salida.\n",
    "\n",
    "\n",
    "---\n",
    "<i><small>**Más infromación** sobre el porqué matemático de la función de pérdida, aunque ya explicado, se puede encontrar en el paper [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) y una explicación más clara en la página [Diffusion Model Clearly Explained!](https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166).\n",
    "\n",
    "<span style=\"color: red; font-size: 1.5em;\">&#9888;</span> **NOTA:** El proceso matemático para llegar a esta fórmula es muy complejo para explicarlo en este notebook. Sin embargo, en el futuro informe se explicará con más detalle. \n",
    "</small></i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensorboard callback for monitoring training progress\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=\"./logs\", histogram_freq=1\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "model = build_ddpm_model()\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=10, callbacks=[tensorboard_callback])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block(x_img: tf.Tensor, x_ts: tf.Tensor, x_label: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"The block of the diffusion model\n",
    "\n",
    "    :param x_img: The image to process\n",
    "    :param x_ts: The time steps to process\n",
    "    :param x_label: The label to process\n",
    "    :return: The processed image\n",
    "    \"\"\"\n",
    "\n",
    "    x_parameter = layers.Conv2D(128, kernel_size=3, padding=\"same\")(x_img)\n",
    "    x_parameter = layers.Activation(\"relu\")(x_parameter)\n",
    "\n",
    "    time_parameter = layers.Dense(128)(x_ts)\n",
    "    time_parameter = layers.Activation(\"relu\")(time_parameter)\n",
    "    time_parameter = layers.Reshape((1, 1, 128))(time_parameter)\n",
    "\n",
    "    label_parameter = layers.Dense(128)(x_label)\n",
    "    label_parameter = layers.Activation(\"relu\")(label_parameter)\n",
    "    label_parameter = layers.Reshape((1, 1, 128))(label_parameter)\n",
    "\n",
    "    x_parameter = x_parameter * label_parameter + time_parameter\n",
    "\n",
    "    # -----\n",
    "    x_out = layers.Conv2D(128, kernel_size=3, padding=\"same\")(x_img)\n",
    "    x_out = x_out + x_parameter\n",
    "    x_out = layers.LayerNormalization()(x_out)\n",
    "    x_out = layers.Activation(\"relu\")(x_out)\n",
    "\n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. El proceso de difusión utiliza una arquitectura de tipo **U-Net** modificada con bloques ``block`` que toman en cuenta la imagen (``x_parameter``), el tensor tiempo (``time_parameter``) y la etiqueta (``label_parameter``). Posteriormente, se realizan operaciones de convolución y pooling para reducir la resolución de la imagen mientras se procesa la información temporal y de etiqueta. Luego, se realiza un proceso de decodificación utilizando operaciones de upsampling y concatenación para generar una imagen de salida que tiene la misma resolución que la imagen de entrada. Después de este proceso, se añade una capa **MLP** para procesar la información temporal y de etiqueta y generar una imagen de salida. Finalmente, se devuelve la imagen de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ddpm_model() -> tf.keras.models.Model:\n",
    "    \"\"\"Creates the diffusion model\n",
    "\n",
    "    :return: The diffusion model\n",
    "    \"\"\"\n",
    "\n",
    "    x = x_input = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3), name=\"x_input\")\n",
    "\n",
    "    x_ts = x_ts_input = layers.Input(shape=(1,), name=\"x_ts_input\")\n",
    "    x_ts = layers.Dense(192)(x_ts)\n",
    "    x_ts = layers.LayerNormalization()(x_ts)\n",
    "    x_ts = layers.Activation(\"relu\")(x_ts)\n",
    "\n",
    "    x_label = x_label_input = layers.Input(shape=(NUM_CLASSES,), name=\"x_label_input\")\n",
    "    x_label = layers.Dense(192)(x_label)\n",
    "    x_label = layers.LayerNormalization()(x_label)\n",
    "    x_label = layers.Activation(\"relu\")(x_label)\n",
    "\n",
    "    # ----- left ( down ) -----\n",
    "    x = x64 = block(x, x_ts, x_label)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "\n",
    "    x = x32 = block(x, x_ts, x_label)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "\n",
    "    x = x16 = block(x, x_ts, x_label)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "\n",
    "    x = x8 = block(x, x_ts, x_label)\n",
    "\n",
    "    # ----- MLP -----\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Concatenate()([x, x_ts, x_label])\n",
    "    x = layers.Dense(128)(x)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = layers.Dense(8 * 8 * 32)(x)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.Reshape((8, 8, 32))(x)\n",
    "\n",
    "    # ----- right ( up ) -----\n",
    "    x = layers.Concatenate()([x, x8])\n",
    "    x = block(x, x_ts, x_label)\n",
    "    x = layers.UpSampling2D(2)(x)\n",
    "\n",
    "    x = layers.Concatenate()([x, x16])\n",
    "    x = block(x, x_ts, x_label)\n",
    "    x = layers.UpSampling2D(2)(x)\n",
    "\n",
    "    x = layers.Concatenate()([x, x32])\n",
    "    x = block(x, x_ts, x_label)\n",
    "    x = layers.UpSampling2D(2)(x)\n",
    "\n",
    "    x = layers.Concatenate()([x, x64])\n",
    "    x = block(x, x_ts, x_label)\n",
    "\n",
    "    # ----- output -----\n",
    "    x = layers.Conv2D(3, kernel_size=1, padding=\"same\")(x)\n",
    "    model = tf.keras.models.Model([x_input, x_ts_input, x_label_input], x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "# =====================================================================\n",
    "model = build_ddpm_model()\n",
    "\n",
    "# Compile the model\n",
    "# =====================================================================\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "model.compile(loss=loss_fn, optimizer=optimizer)\n",
    "\n",
    "# Show the model summary\n",
    "# =====================================================================\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, show_shapes=True, show_layer_names=True, dpi=96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[BACK TO TOP](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
