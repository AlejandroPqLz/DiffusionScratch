{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pokémon Diffusion<a id=\"top\"></a>**\n",
    "\n",
    "> #### ``04-Training-Diffusion-Model.ipynb``\n",
    "\n",
    "<i><small>**Alumno:** Alejandro Pequeño Lizcano<br>Última actualización: 11/03/2024</small></i></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: INTRODUCIR MEJOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Como paso final, se procede a entrenar el modelo de difusión. Para ello, se ha definido la función ``training()`` que engloba todo el proceso de difusión completo, tanto hacia adelante como hacia atrás y los ploteos de las muestras generadas. Para implementar el training hemos usado el **Algoritmo 1** de [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) y se ha modificado para que sea capaz de generar imágenes condicionadas a una etiqueta.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<img src=\"../figures/notebook_figures/algorithm1_training.png\" width=\"40%\" height=\"30%\" />\n",
    "</div>\n",
    "\n",
    "Tambiñen se han añadido unas funcionalidades extra que permiten guardar cada epoch el modelo y sus pesos en un fichero con extensión .h5. Esto se hace para poder cargar el modelo y continuar el entrenamiento desde donde se quedó en caso de que se interrumpa por algún motivo.\n",
    "\n",
    "<span style=\"color: red; font-size: 1.5em;\">&#9888;</span> <i><small>**NOTA:** Por cada epoch se guarda en un fichero con extensión .h5 tanto el modelo como sus pesos. Este proceso se realiza ya que todo el entrenamiento es muy costoso y si se interrumpe por algún motivo, se puede volver a cargar el modelo y continuar el entrenamiento desde donde se quedó.\n",
    "\n",
    "También cabe destacar que para una mayor eficiencia en el entrenamiento, se ha optado por realizar el ``sampling()`` cada 5 epochs.\n",
    "</small></i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 1: Training\n",
    "# =====================================================================\n",
    "def training(\n",
    "    model: tf.keras.models.Model,\n",
    "    dataset: tf.data.Dataset,\n",
    "    optimizer: tf.keras.optimizers.Optimizer,\n",
    "    loss_fn: tf.keras.losses.Loss,\n",
    "    total_epochs: int = 10,\n",
    "    scheduler: str = \"cosine\",\n",
    "    T: int = 100,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Performs the training loop.\n",
    "\n",
    "    :param model: The model to train.\n",
    "    :param dataset: The training dataset.\n",
    "    :param optimizer: The optimizer to use.\n",
    "    :param loss_fn: The loss function to use.\n",
    "    :param total_epochs: The number of epochs to train for.\n",
    "    :param scheduler: The type of schedule to use. Options are \"linear\" or \"cosine\".\n",
    "    :param T: The number of timesteps to sample for.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    # Save intermodels by epoch\n",
    "    # =====================================================================\n",
    "\n",
    "    # Create the folder to save the models by epoch\n",
    "    folder_epoch = f\"../../models/inter_models/diffusion_{IMG_SIZE}_{BATCH_SIZE}_{EPOCHS}_{T}_{scheduler}_ddpm\"\n",
    "    if not os.path.exists(folder_epoch):\n",
    "        os.makedirs(folder_epoch)\n",
    "\n",
    "    # Check if there are checkpoints to load\n",
    "    if len(glob.glob(f\"{folder_epoch}/diffusion_{scheduler}_*.h5\")) > 0:\n",
    "\n",
    "        last_checkpoint = sorted(\n",
    "            glob.glob(f\"{folder_epoch}/diffusion_{scheduler}*.h5\"),\n",
    "            key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]),\n",
    "        )[\n",
    "            -1\n",
    "        ]  # Get the last checkpoint\n",
    "        print(f\"Loading checkpoint {last_checkpoint}...\")\n",
    "\n",
    "        # Get the epoch from the checkpoint\n",
    "        prev_epoch = int(\n",
    "            last_checkpoint.split(\"_\")[-1].split(\".\")[0]\n",
    "        )  # Get the epoch from the checkpoint\n",
    "        print(f\"Resuming training from epoch {prev_epoch}...\")\n",
    "\n",
    "        model = tf.keras.models.load_model(\n",
    "            f\"{folder_epoch}/diffusion_{scheduler}_{prev_epoch}.h5\"\n",
    "        )  # Load the model\n",
    "\n",
    "    else:\n",
    "        prev_epoch = 0\n",
    "        print(\"No checkpoints found, starting training from scratch...\")\n",
    "\n",
    "    # Start the training loop\n",
    "    # =====================================================================\n",
    "\n",
    "    # Get scheduler values\n",
    "    beta = beta_scheduler(scheduler, T, beta_start, beta_end)  # Get beta\n",
    "    alpha = 1.0 - beta  # Get alpha\n",
    "    alpha_cumprod = np.cumprod(alpha)  # Get alpha cumulative product\n",
    "\n",
    "    for epoch in trange(\n",
    "        prev_epoch,\n",
    "        total_epochs,\n",
    "        desc=f\"Training\",\n",
    "        total=total_epochs - prev_epoch,\n",
    "        leave=True,\n",
    "    ):  # 1: repeat (iterations through the epochs)\n",
    "        for step, input_data in tqdm(\n",
    "            enumerate(dataset),\n",
    "            desc=f\"Epoch {epoch+1}/{total_epochs}\",\n",
    "            total=len(dataset),\n",
    "            leave=True,\n",
    "        ):  # 1: repeat (iterations through the batches)\n",
    "            # Generate a single timestep for one entire batch\n",
    "            t = np.random.randint(0, T)\n",
    "            normalized_t = np.full(\n",
    "                (input_data.shape[0], 1), t / T, dtype=np.float32\n",
    "            )  # 3: t ~ U(0, T)\n",
    "\n",
    "            # Get the target noise\n",
    "            noised_data = forward_diffusion(input_data, t, scheduler)  # 2: x_0 ~ q(x_0)\n",
    "            target_noise = noised_data - input_data * np.sqrt(\n",
    "                alpha_cumprod[t]\n",
    "            )  # 4: eps_t ~ N(0, I)\n",
    "\n",
    "            # 5: Take a gradient descent step on\n",
    "            with tf.GradientTape() as tape:\n",
    "                predicted_noise = model(\n",
    "                    [noised_data, normalized_t], training=True\n",
    "                )  # eps_theta -> model(x_t, t/T)\n",
    "                loss = loss_fn(\n",
    "                    target_noise, predicted_noise\n",
    "                )  # gradient of the loss (MSE(eps_t, eps_theta))\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        print(f\"EPOCH {epoch+1} LOSS: {loss.numpy():.4f} \\n{'='*69}\")\n",
    "\n",
    "        # Save the model at the end of every 20 epochs\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"\\tSaving model in epoch {epoch+1}\")\n",
    "            model.save(f\"{folder_epoch}/diffusion_{scheduler}_{epoch+1}.h5\")\n",
    "\n",
    "        # Sample and plot every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(\"\\tSampling images...\")\n",
    "            plot_samples(model, num_samples=3, scheduler=scheduler, T=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# =====================================================================\n",
    "training(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    scheduler=\"cosine\",\n",
    "    num_epochs=EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model\n",
    "\n",
    "Finalmente, se guardan los resultados finales del modelo de difusión en un fichero `.h5` para su posterior uso y visualización. TODO: MIRAR OTROS FORMATOS DE GUARDADO\n",
    "\n",
    "TODO: INVESTIGAR OTROS FORMATOS DE GUARDADO (HDF5, PICKLE, ETC.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model function\n",
    "# =====================================================================\n",
    "def save_model(model: tf.keras.models.Model, model_name: str) -> None:\n",
    "    \"\"\"Saves the model\n",
    "\n",
    "    :param model: The model to save\n",
    "    :param model_name: The name of the model\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    # Save the model\n",
    "    model_dir = \"./diffusion_models/models/\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    if not os.path.exists(os.path.join(model_dir, f\"{model_name}.h5\")):\n",
    "        model.save(os.path.join(model_dir, f\"{model_name}.h5\"))\n",
    "        print(f\"Model {model_name}, saved successfully!\")\n",
    "    else:\n",
    "        print(f\"Model {model_name}, already exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_name = f\"diffusion_{IMG_SIZE}_{BATCH_SIZE}_{EPOCHS}_{T}_{scheduler}_ddpm\"\n",
    "\n",
    "save_model(model, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[BACK TO TOP](#top)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
